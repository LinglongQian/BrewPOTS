{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“‘ Tutorials for PyPOTS Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“€ Preparing the **PhysioNet-2012** dataset for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 22:49:34 [INFO]: Have set the random seed as 16 for numpy and pytorch.\n",
      "2024-03-17 22:49:34 [INFO]: Loading the dataset physionet_2012 with TSDB (https://github.com/WenjieDu/Time_Series_Data_Beans)...\n",
      "2024-03-17 22:49:34 [INFO]: Starting preprocessing physionet_2012...\n",
      "2024-03-17 22:49:34 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: \n",
      "https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012\n",
      "2024-03-17 22:49:34 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...\n",
      "2024-03-17 22:49:34 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...\n",
      "2024-03-17 22:49:34 [INFO]: Loaded successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['n_classes', 'n_steps', 'n_features', 'train_X', 'train_y', 'train_ICUType', 'val_X', 'val_y', 'val_ICUType', 'test_X', 'test_y', 'test_ICUType', 'scaler', 'val_X_ori', 'test_X_ori', 'test_X_indicating_mask'])\n"
     ]
    }
   ],
   "source": [
    "from pypots.data.generating import gene_physionet2012\n",
    "from pypots.utils.random import set_random_seed\n",
    "from global_config import RANDOM_SEED\n",
    "\n",
    "set_random_seed(RANDOM_SEED)\n",
    "\n",
    "# Load the PhysioNet-2012 dataset, disable artificially-missing values for evaluation\n",
    "physionet2012_dataset = gene_physionet2012(artificially_missing_rate=0.1)\n",
    " \n",
    "# Take a look at the generated PhysioNet-2012 dataset, you'll find that everything has been prepared for you, \n",
    "# data splitting, normalization, additional artificially-missing values for evaluation, etc.\n",
    "print(physionet2012_dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the datasets for training, validating, and testing.\n",
    "\n",
    "dataset_for_training = {\n",
    "    \"X\": physionet2012_dataset['train_X'],\n",
    "    \"y\": physionet2012_dataset['train_y'],\n",
    "}\n",
    "\n",
    "dataset_for_validating = {\n",
    "    \"X\": physionet2012_dataset['val_X'],\n",
    "    \"y\": physionet2012_dataset['val_y'],\n",
    "}\n",
    "\n",
    "dataset_for_testing = {\n",
    "    \"X\": physionet2012_dataset['test_X'],\n",
    "    \"y\": physionet2012_dataset['test_y'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ An example of **Raindrop** for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 22:49:51 [INFO]: Using the given device: cuda\n",
      "2024-03-17 22:49:51 [INFO]: Model files will be saved to tutorial_results/classification/raindrop/20240317_T224951\n",
      "2024-03-17 22:49:51 [INFO]: Tensorboard file will be saved to tutorial_results/classification/raindrop/20240317_T224951/tensorboard\n",
      "/home/wdudu/.conda/envs/ml/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "2024-03-17 22:49:52 [INFO]: Raindrop initialized with the given hyperparameters, the number of trainable parameters: 1,415,006\n"
     ]
    }
   ],
   "source": [
    "from pypots.optim import Adam\n",
    "from pypots.classification import Raindrop\n",
    "\n",
    "# initialize the model\n",
    "raindrop = Raindrop(\n",
    "    n_steps=physionet2012_dataset['n_steps'],\n",
    "    n_features=physionet2012_dataset['n_features'],\n",
    "    n_classes=physionet2012_dataset[\"n_classes\"],\n",
    "    n_layers=2,\n",
    "    d_model=physionet2012_dataset[\"n_features\"] * 4,\n",
    "    d_ffn=256,\n",
    "    n_heads=2,\n",
    "    dropout=0.3,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,  \n",
    "    # set the path for saving tensorboard and trained model files \n",
    "    saving_path=\"../tutorial_results/classification/raindrop\",\n",
    "    model_saving_strategy=\"best\", # only save the best model after training finished. \n",
    "                                  # You can also set it as \"better\" to save models performing better ever during training.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 22:50:23 [INFO]: Epoch 001 - training loss: 0.3865, validating loss: 0.3650\n",
      "2024-03-17 22:50:47 [INFO]: Epoch 002 - training loss: 0.3357, validating loss: 0.3486\n",
      "2024-03-17 22:51:10 [INFO]: Epoch 003 - training loss: 0.3231, validating loss: 0.3519\n",
      "2024-03-17 22:51:33 [INFO]: Epoch 004 - training loss: 0.3168, validating loss: 0.3371\n",
      "2024-03-17 22:51:55 [INFO]: Epoch 005 - training loss: 0.3094, validating loss: 0.3402\n",
      "2024-03-17 22:52:17 [INFO]: Epoch 006 - training loss: 0.3067, validating loss: 0.3556\n",
      "2024-03-17 22:52:39 [INFO]: Epoch 007 - training loss: 0.2993, validating loss: 0.3314\n",
      "2024-03-17 22:53:07 [INFO]: Epoch 008 - training loss: 0.2898, validating loss: 0.3399\n",
      "2024-03-17 22:53:35 [INFO]: Epoch 009 - training loss: 0.2822, validating loss: 0.3308\n",
      "2024-03-17 22:53:59 [INFO]: Epoch 010 - training loss: 0.2759, validating loss: 0.3503\n",
      "2024-03-17 22:53:59 [INFO]: Finished training.\n",
      "2024-03-17 22:53:59 [INFO]: Saved the model to tutorial_results/classification/raindrop/20240317_T224951/Raindrop.pypots\n"
     ]
    }
   ],
   "source": [
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "raindrop.fit(train_set=dataset_for_training, val_set=dataset_for_validating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the testing stage\n",
    "raindrop_results = raindrop.predict(dataset_for_testing)\n",
    "raindrop_prediction = raindrop_results[\"classification\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing classification metrics: \n",
      "ROC_AUC: 0.8110672840564797, \n",
      "PR_AUC: 0.4184412610743794,\n",
      "F1: 0.41390205371248023,\n",
      "Precision: 0.447098976109215,\n",
      "Recall: 0.38529411764705884,\n"
     ]
    }
   ],
   "source": [
    "from pypots.utils.metrics import calc_binary_classification_metrics\n",
    "\n",
    "# calculate the values of binary classification metrics on the model's prediction\n",
    "metrics = calc_binary_classification_metrics(raindrop_prediction, dataset_for_testing[\"y\"])\n",
    "print(\"Testing classification metrics: \\n\"\n",
    "    f'ROC_AUC: {metrics[\"roc_auc\"]}, \\n'\n",
    "    f'PR_AUC: {metrics[\"pr_auc\"]},\\n'\n",
    "    f'F1: {metrics[\"f1\"]},\\n'\n",
    "    f'Precision: {metrics[\"precision\"]},\\n'\n",
    "    f'Recall: {metrics[\"recall\"]},\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ An example of **BRITS** for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 22:54:03 [INFO]: Using the given device: cuda\n",
      "2024-03-17 22:54:03 [INFO]: Model files will be saved to tutorial_results/classification/brits/20240317_T225403\n",
      "2024-03-17 22:54:03 [INFO]: Tensorboard file will be saved to tutorial_results/classification/brits/20240317_T225403/tensorboard\n",
      "2024-03-17 22:54:03 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 730,612\n"
     ]
    }
   ],
   "source": [
    "from pypots.optim import Adam\n",
    "from pypots.classification import BRITS\n",
    "\n",
    "# initialize the model\n",
    "brits = BRITS(\n",
    "    n_steps=physionet2012_dataset['n_steps'], \n",
    "    n_features=physionet2012_dataset['n_features'], \n",
    "    n_classes=physionet2012_dataset[\"n_classes\"],\n",
    "    rnn_hidden_size=256,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,  \n",
    "    # set the path for saving tensorboard and trained model files \n",
    "    saving_path=\"../tutorial_results/classification/brits\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 22:55:09 [INFO]: Epoch 001 - training loss: 0.9035, validating loss: 0.8116\n",
      "2024-03-17 22:56:04 [INFO]: Epoch 002 - training loss: 0.7604, validating loss: 0.7751\n",
      "2024-03-17 22:57:10 [INFO]: Epoch 003 - training loss: 0.7164, validating loss: 0.7471\n",
      "2024-03-17 22:58:17 [INFO]: Epoch 004 - training loss: 0.6902, validating loss: 0.7711\n",
      "2024-03-17 22:59:23 [INFO]: Epoch 005 - training loss: 0.6782, validating loss: 0.7286\n",
      "2024-03-17 23:00:31 [INFO]: Epoch 006 - training loss: 0.6547, validating loss: 0.7265\n",
      "2024-03-17 23:01:57 [INFO]: Epoch 007 - training loss: 0.6419, validating loss: 0.7295\n",
      "2024-03-17 23:03:49 [INFO]: Epoch 008 - training loss: 0.6221, validating loss: 0.7234\n",
      "2024-03-17 23:05:36 [INFO]: Epoch 009 - training loss: 0.6007, validating loss: 0.7549\n",
      "2024-03-17 23:07:16 [INFO]: Epoch 010 - training loss: 0.5942, validating loss: 0.7689\n",
      "2024-03-17 23:07:16 [INFO]: Finished training.\n",
      "2024-03-17 23:07:16 [INFO]: Saved the model to tutorial_results/classification/brits/20240317_T225403/BRITS.pypots\n"
     ]
    }
   ],
   "source": [
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "brits.fit(train_set=dataset_for_training, val_set=dataset_for_validating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the testing stage\n",
    "brits_results = brits.predict(dataset_for_testing)\n",
    "brits_prediction = brits_results[\"classification\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing classification metrics: \n",
      "ROC_AUC: 0.811434573829532, \n",
      "PR_AUC: 0.40568596808086554,\n",
      "F1: 0.3706293706293706,\n",
      "Precision: 0.45689655172413796,\n",
      "Recall: 0.31176470588235294,\n"
     ]
    }
   ],
   "source": [
    "from pypots.utils.metrics import calc_binary_classification_metrics\n",
    "\n",
    "# calculate the values of binary classification metrics on the model's prediction\n",
    "metrics = calc_binary_classification_metrics(brits_prediction, dataset_for_testing[\"y\"])\n",
    "print(\"Testing classification metrics: \\n\"\n",
    "    f'ROC_AUC: {metrics[\"roc_auc\"]}, \\n'\n",
    "    f'PR_AUC: {metrics[\"pr_auc\"]},\\n'\n",
    "    f'F1: {metrics[\"f1\"]},\\n'\n",
    "    f'Precision: {metrics[\"precision\"]},\\n'\n",
    "    f'Recall: {metrics[\"recall\"]},\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ An example of **GRUD** for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 23:07:31 [INFO]: Using the given device: cuda\n",
      "2024-03-17 23:07:31 [INFO]: Model files will be saved to tutorial_results/classification/grud/20240317_T230731\n",
      "2024-03-17 23:07:31 [INFO]: Tensorboard file will be saved to tutorial_results/classification/grud/20240317_T230731/tensorboard\n",
      "2024-03-17 23:07:31 [INFO]: GRUD initialized with the given hyperparameters, the number of trainable parameters: 16,128\n"
     ]
    }
   ],
   "source": [
    "from pypots.optim import Adam\n",
    "from pypots.classification import GRUD\n",
    "\n",
    "# initialize the model\n",
    "grud = GRUD(\n",
    "    n_steps=physionet2012_dataset['n_steps'], \n",
    "    n_features=physionet2012_dataset['n_features'], \n",
    "    n_classes=physionet2012_dataset[\"n_classes\"],\n",
    "    rnn_hidden_size=32, \n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,  \n",
    "    # set the path for saving tensorboard and trained model files \n",
    "    saving_path=\"../tutorial_results/classification/grud\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 23:07:55 [INFO]: Epoch 001 - training loss: 0.3332, validating loss: 0.3188\n",
      "2024-03-17 23:08:15 [INFO]: Epoch 002 - training loss: 0.2950, validating loss: 0.3212\n",
      "2024-03-17 23:08:35 [INFO]: Epoch 003 - training loss: 0.2855, validating loss: 0.3182\n",
      "2024-03-17 23:08:56 [INFO]: Epoch 004 - training loss: 0.2779, validating loss: 0.3223\n",
      "2024-03-17 23:09:18 [INFO]: Epoch 005 - training loss: 0.2711, validating loss: 0.3155\n",
      "2024-03-17 23:09:39 [INFO]: Epoch 006 - training loss: 0.2619, validating loss: 0.3330\n",
      "2024-03-17 23:10:00 [INFO]: Epoch 007 - training loss: 0.2571, validating loss: 0.3182\n",
      "2024-03-17 23:10:21 [INFO]: Epoch 008 - training loss: 0.2509, validating loss: 0.3229\n",
      "2024-03-17 23:10:21 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2024-03-17 23:10:21 [INFO]: Finished training.\n",
      "2024-03-17 23:10:21 [INFO]: Saved the model to tutorial_results/classification/grud/20240317_T230731/GRUD.pypots\n"
     ]
    }
   ],
   "source": [
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "grud.fit(train_set=dataset_for_training, val_set=dataset_for_validating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the testing stage\n",
    "grud_results = grud.predict(dataset_for_testing)\n",
    "grud_prediction = grud_results[\"classification\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing classification metrics: \n",
      "ROC_AUC: 0.8327902589607271, \n",
      "PR_AUC: 0.48160561802559804,\n",
      "F1: 0.41064638783269963,\n",
      "Precision: 0.5806451612903226,\n",
      "Recall: 0.3176470588235294,\n"
     ]
    }
   ],
   "source": [
    "from pypots.utils.metrics import calc_binary_classification_metrics\n",
    "\n",
    "# calculate the values of binary classification metrics on the model's prediction\n",
    "metrics = calc_binary_classification_metrics(grud_prediction, dataset_for_testing[\"y\"])\n",
    "print(\"Testing classification metrics: \\n\"\n",
    "    f'ROC_AUC: {metrics[\"roc_auc\"]}, \\n'\n",
    "    f'PR_AUC: {metrics[\"pr_auc\"]},\\n'\n",
    "    f'F1: {metrics[\"f1\"]},\\n'\n",
    "    f'Precision: {metrics[\"precision\"]},\\n'\n",
    "    f'Recall: {metrics[\"recall\"]},\\n'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
