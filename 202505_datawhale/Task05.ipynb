{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ba919c9-ff72-42ea-867e-fa6013b16810",
   "metadata": {},
   "source": [
    "# Task05. 自定义时序数据集的预处理与插补"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed483ad5-fd01-42c7-a629-f9fcd5db3b89",
   "metadata": {},
   "source": [
    "在本节中，我们将以**合成的 eICU 数据集**为例，演示如何将自定义的医疗时间序列数据预处理为 [PyPOTS](https://github.com/WenjieDu/PyPOTS) 框架所需的输入格式，并使用 PyPOTS 进行插补。\n",
    "\n",
    "## 关于 eICU 数据集\n",
    "\n",
    "> The eICU Collaborative Research Database is a freely available multi-center database for critical care research.  \n",
    "> **Reference**:  \n",
    "> Pollard TJ, Johnson AEW, Raffa JD, Celi LA, Mark RG, and Badawi O. (2018). *The eICU Collaborative Research Database: A multi-center critical care database for research*. Scientific Data. DOI: [10.1038/sdata.2018.178](http://dx.doi.org/10.1038/sdata.2018.178)  \n",
    "> Available at: [https://www.nature.com/articles/sdata2018178](https://www.nature.com/articles/sdata2018178)\n",
    "\n",
    "eICU 数据库包含来自多家医院的 ICU 病患监护记录，是医疗时间序列研究的重要开源资源。在本示例中，我们使用经过脱敏和合成的 eICU 数据集，以避免隐私风险，同时保证数据结构与真实医疗数据一致。\n",
    "\n",
    "## 任务目标\n",
    "\n",
    "- 预处理表格格式的医疗时序数据为 PyPOTS 可用格式。\n",
    "- 使用 PyPOTS 进行插补并还原数据。\n",
    "- 生成可供后续分析或模型训练的数据集。\n",
    "\n",
    "## 主要步骤\n",
    "\n",
    "1. **数据加载**  \n",
    "   加载原始时序数据，包括特征、标签和样本标识。\n",
    "\n",
    "2. **构建三维张量**  \n",
    "   - 将不同样本的特征对齐到统一的时间步长度。\n",
    "   - 构造三维张量 `(n_samples, n_steps, n_features)`。\n",
    "\n",
    "3. **数据插补**  \n",
    "   使用 PyPOTS 提供的插补算法对张量中的缺失值进行填充。\n",
    "\n",
    "4. **还原 DataFrame 结构**  \n",
    "   将插补后的张量转换回 DataFrame 形式，保留样本 ID、时间步、特征和标签。\n",
    "\n",
    "## 结果说明\n",
    "\n",
    "执行完以上步骤后，你将得到三个预处理完成的数据集：\n",
    "- `df_train_imputed`：训练集插补结果\n",
    "- `df_val_imputed`：验证集插补结果\n",
    "- `df_test_imputed`：测试集插补结果\n",
    "\n",
    "## 示例输出检查\n",
    "\n",
    "通过 `.shape` 查看数据集维度，确认处理无误：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3f65cc",
   "metadata": {},
   "source": [
    "### 1. 自定义时序数据集的预处理与插补"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "174b64a9-e4e9-4dca-b2bb-428b1478476e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/miniconda3/envs/workshop/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗\n",
      "╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║\n",
      "   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║\n",
      "   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║\n",
      "   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║\n",
      "   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝\n",
      "ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai \u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pypots\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tsdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from benchpots.utils.logging import logger, print_final_dataset_info\n",
    "from benchpots.utils.missingness import create_missingness # 生成人工缺失值\n",
    "\n",
    "# 设置模型的运行设备为cpu, 如果你有gpu设备可以设置为cuda\n",
    "DEVICE='cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c99536",
   "metadata": {},
   "source": [
    "### 1.1 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f042d9c-a5da-4ee0-9e5d-966951d5292f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>apacheadmissiondx</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>gender</th>\n",
       "      <th>GCS Total</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Motor</th>\n",
       "      <th>Verbal</th>\n",
       "      <th>admissionheight</th>\n",
       "      <th>...</th>\n",
       "      <th>MAP (mmHg)</th>\n",
       "      <th>Invasive BP Diastolic</th>\n",
       "      <th>Invasive BP Systolic</th>\n",
       "      <th>O2 Saturation</th>\n",
       "      <th>Respiratory Rate</th>\n",
       "      <th>Temperature (C)</th>\n",
       "      <th>glucose</th>\n",
       "      <th>FiO2</th>\n",
       "      <th>pH</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>182.9</td>\n",
       "      <td>...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>182.9</td>\n",
       "      <td>...</td>\n",
       "      <td>79.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>182.9</td>\n",
       "      <td>...</td>\n",
       "      <td>75.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>17.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>182.9</td>\n",
       "      <td>...</td>\n",
       "      <td>79.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>17.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>182.9</td>\n",
       "      <td>...</td>\n",
       "      <td>76.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id  timestamp  apacheadmissiondx  ethnicity  gender  GCS Total  \\\n",
       "0          0          0               17.0      394.0   398.0        NaN   \n",
       "1          0          1               17.0      394.0   398.0        NaN   \n",
       "2          0          2               17.0      394.0   398.0      413.0   \n",
       "3          0          3               17.0      394.0   398.0        NaN   \n",
       "4          0          4               17.0      394.0   398.0        NaN   \n",
       "\n",
       "   Eyes  Motor  Verbal  admissionheight  ...  MAP (mmHg)  \\\n",
       "0   NaN    NaN     NaN            182.9  ...        80.0   \n",
       "1   NaN    NaN     NaN            182.9  ...        79.0   \n",
       "2   NaN    NaN     NaN            182.9  ...        75.0   \n",
       "3   NaN    NaN     NaN            182.9  ...        79.0   \n",
       "4   NaN    NaN     NaN            182.9  ...        76.0   \n",
       "\n",
       "   Invasive BP Diastolic  Invasive BP Systolic  O2 Saturation  \\\n",
       "0                   56.0                 119.0           99.0   \n",
       "1                   56.0                 112.0           98.0   \n",
       "2                   56.0                 112.0           98.0   \n",
       "3                   58.0                 108.0           97.0   \n",
       "4                   55.0                 111.0           91.0   \n",
       "\n",
       "   Respiratory Rate  Temperature (C)  glucose  FiO2  pH  label  \n",
       "0               NaN              NaN      NaN   NaN NaN      0  \n",
       "1               NaN              NaN      NaN   NaN NaN      0  \n",
       "2              20.0             35.3      NaN   NaN NaN      0  \n",
       "3               NaN              NaN      NaN   NaN NaN      0  \n",
       "4               NaN              NaN      NaN   NaN NaN      0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('synthetic_eicu.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f2c9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_112726/983142174.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  new_df = df.groupby('sample_id').apply(pad_truncate).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "确保时间步长的一致性：\n",
    "如果自定义数据的时间序列长度不一，则需要通过用缺失值 (NaN) 填充较短的序列或截断较长的序列来对其进行标准化。\n",
    "我们来设置一个最大长度，例如，我们有 48 个时间步长，表示每个患者 48 小时的记录（可以根据数据进行调整）。\n",
    "'''\n",
    "\n",
    "max_length = 48\n",
    "\n",
    "def pad_truncate(df):\n",
    "    if len(df) > max_length:\n",
    "        # 如果 DataFrame 超过最大长度，则截断\n",
    "        # 这里我们选择保留前 max_length 行\n",
    "        # 你也可以选择其他策略，比如保留最后 max_length 行\n",
    "        return df.iloc[:max_length]\n",
    "    else:\n",
    "        # 如果 DataFrame 少于最大长度，则填充\n",
    "        # 这里我们用 NaN 填充\n",
    "        # 你也可以选择其他填充值，比如 0 或者均值等\n",
    "        padding = pd.DataFrame(\n",
    "            index=range(max_length - len(df)),\n",
    "            columns=df.columns\n",
    "        )\n",
    "        if not padding.empty:\n",
    "            return pd.concat([df, padding])\n",
    "        else:\n",
    "            return df\n",
    "\n",
    "# 对每个患者的时间序列进行填充或截断\n",
    "# 这里假设 'sample_id' 是患者的唯一标识符\n",
    "# 你需要根据你的数据集中的实际列名进行调整\n",
    "new_df = df.groupby('sample_id').apply(pad_truncate).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb71c3",
   "metadata": {},
   "source": [
    "### 1.2 数据拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f9c620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame shape: (235584, 23)\n",
      "Validation DataFrame shape: (29472, 23)\n",
      "Test DataFrame shape: (29472, 23)\n",
      "Train features shape: (4908, 48, 20), Train labels shape: (4908,)\n",
      "Validation features shape: (614, 48, 20), Validation labels shape: (614,)\n",
      "Test features shape: (614, 48, 20), Test labels shape: (614,)\n"
     ]
    }
   ],
   "source": [
    "unique_sample_ids = new_df['sample_id'].unique()\n",
    "\n",
    "train_ids, temp_ids = train_test_split(unique_sample_ids, test_size=0.2, random_state=42)\n",
    "val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=42)\n",
    "\n",
    "train_df = new_df[new_df['sample_id'].isin(train_ids)]\n",
    "val_df = new_df[new_df['sample_id'].isin(val_ids)]\n",
    "test_df = new_df[new_df['sample_id'].isin(test_ids)]\n",
    "\n",
    "print(f\"Train DataFrame shape: {train_df.shape}\")\n",
    "print(f\"Validation DataFrame shape: {val_df.shape}\")\n",
    "print(f\"Test DataFrame shape: {test_df.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "# 拆分特征和标签\n",
    "\n",
    "def separate_features_labels(df, feature_cols, label_col='label'):\n",
    "    X = df[feature_cols].values.reshape(-1, 48, len(feature_cols))\n",
    "    # 获取唯一的样本 ID\n",
    "    unique_ids = df['sample_id'].unique()\n",
    "    # 获取每个样本 ID 的第一个标签\n",
    "    y = df.groupby('sample_id')[label_col].first().loc[unique_ids].values\n",
    "    return X, y\n",
    "\n",
    "# 选择特征列\n",
    "feature_columns = [col for col in df.columns if col not in ['sample_id', 'label', 'timestamp']]\n",
    "\n",
    "train_X, train_y = separate_features_labels(train_df.copy(), feature_columns)\n",
    "val_X, val_y = separate_features_labels(val_df.copy(), feature_columns)\n",
    "test_X, test_y = separate_features_labels(test_df.copy(), feature_columns)\n",
    "\n",
    "print(f\"Train features shape: {train_X.shape}, Train labels shape: {train_y.shape}\")\n",
    "print(f\"Validation features shape: {val_X.shape}, Validation labels shape: {val_y.shape}\")\n",
    "print(f\"Test features shape: {test_X.shape}, Test labels shape: {test_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17262d25",
   "metadata": {},
   "source": [
    "### 1.3 数据标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cb25725",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# Flatten the data before scaling and then reshape it into time series samples\n",
    "train_X = scaler.fit_transform(train_X.reshape(-1, train_X.shape[-1])).reshape(train_X.shape)\n",
    "val_X = scaler.transform(val_X.reshape(-1, val_X.shape[-1])).reshape(val_X.shape)\n",
    "test_X = scaler.transform(test_X.reshape(-1, test_X.shape[-1])).reshape(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eec01125",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = {\n",
    "        # general info\n",
    "        \"n_classes\": len(np.unique(train_y)),\n",
    "        \"n_steps\": train_X.shape[-2],\n",
    "        \"n_features\": train_X.shape[-1],\n",
    "        \"scaler\": scaler,\n",
    "        # train set\n",
    "        \"train_X\": train_X,\n",
    "        \"train_y\": train_y.flatten(),\n",
    "        # val set\n",
    "        \"val_X\": val_X,\n",
    "        \"val_y\": val_y.flatten(),\n",
    "        # test set\n",
    "        \"test_X\": test_X,\n",
    "        \"test_y\": test_y.flatten(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdf5f7d",
   "metadata": {},
   "source": [
    "### 1.4 创建人工缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa8121c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保留原始数据中的ground truth以用于评估\n",
    "train_X_ori = train_X\n",
    "val_X_ori = val_X\n",
    "test_X_ori = test_X\n",
    "\n",
    "rate = 0.1 # 10%缺失率\n",
    "\n",
    "# 在训练集上创建缺失值作为ground truth\n",
    "train_X = create_missingness(train_X, rate, 'point')\n",
    "\n",
    "# 在验证集上创建缺失值作为ground truth\n",
    "val_X = create_missingness(val_X, rate, 'point' )\n",
    "\n",
    "# 在测试集上创建缺失值作为ground truth\n",
    "test_X = create_missingness(test_X, rate, 'point' )\n",
    "\n",
    "\n",
    "processed_dataset[\"train_X\"] = train_X\n",
    "processed_dataset[\"val_X\"] = val_X\n",
    "processed_dataset[\"test_X\"] = test_X\n",
    "\n",
    "processed_dataset['train_X_ori'] = train_X_ori\n",
    "processed_dataset['val_X_ori'] = val_X_ori\n",
    "processed_dataset['test_X_ori'] = test_X_ori"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d154412",
   "metadata": {},
   "source": [
    "### 1.5 准备用于插补的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ff474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算掩码来指示X_ori数据中的真实位置，将被用来评估模型性能\n",
    "\n",
    "train_X_indicating_mask = np.isnan(train_X_ori) ^ np.isnan(train_X)\n",
    "val_X_indicating_mask = np.isnan(val_X_ori) ^ np.isnan(val_X)\n",
    "test_X_indicating_mask = np.isnan(test_X_ori) ^ np.isnan(test_X)\n",
    "\n",
    "# 组装训练集\n",
    "dataset_for_training = {\n",
    "    \"X\": processed_dataset['train_X'],\n",
    "    'X_ori': processed_dataset['train_X_ori'],\n",
    "}\n",
    "\n",
    "# 组装验证集\n",
    "dataset_for_validating = {\n",
    "    \"X\": processed_dataset['val_X'],\n",
    "    \"X_ori\": processed_dataset['val_X_ori'],\n",
    "}\n",
    "\n",
    "# 组装测试集\n",
    "dataset_for_testing = {\n",
    "    \"X\": processed_dataset['test_X'],\n",
    "    \"X_ori\": processed_dataset['test_X_ori'],\n",
    "  }\n",
    "\n",
    "test_X_indicating_mask = np.isnan(processed_dataset['test_X_ori']) ^ np.isnan(processed_dataset['test_X'])\n",
    "\n",
    "# 度量函数不接受 NaN 输入，因此用 0 填充 NaN\n",
    "test_X_ori = np.nan_to_num(processed_dataset['test_X_ori'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dccc3e-d043-45c9-8876-2a08dd889817",
   "metadata": {},
   "source": [
    "# 2. 使用SAITS对自定义数据集中的缺失值进行插补"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ec5591",
   "metadata": {},
   "source": [
    "### 2.1 插补数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28f29575-7f31-4791-8350-4220d242f768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 07:34:32 [INFO]: Using the given device: cuda\n",
      "2025-05-10 07:34:32 [INFO]: Model files will be saved to tutorial_results/imputation/saits/20250510_T073432\n",
      "2025-05-10 07:34:32 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/saits/20250510_T073432/tensorboard\n",
      "2025-05-10 07:34:32 [INFO]: Using customized MAE as the training loss function.\n",
      "2025-05-10 07:34:32 [INFO]: Using customized MSE as the validation metric function.\n",
      "2025-05-10 07:34:32 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 691,248\n",
      "2025-05-10 07:34:34 [INFO]: Epoch 001 - training loss (MAE): 0.7562, validation MSE: 0.2322\n",
      "2025-05-10 07:34:37 [INFO]: Epoch 002 - training loss (MAE): 0.4790, validation MSE: 0.2085\n",
      "2025-05-10 07:34:39 [INFO]: Epoch 003 - training loss (MAE): 0.4294, validation MSE: 0.1969\n",
      "2025-05-10 07:34:42 [INFO]: Epoch 004 - training loss (MAE): 0.4008, validation MSE: 0.1893\n",
      "2025-05-10 07:34:45 [INFO]: Epoch 005 - training loss (MAE): 0.3856, validation MSE: 0.1841\n",
      "2025-05-10 07:34:47 [INFO]: Epoch 006 - training loss (MAE): 0.3709, validation MSE: 0.1851\n",
      "2025-05-10 07:34:50 [INFO]: Epoch 007 - training loss (MAE): 0.3610, validation MSE: 0.1796\n",
      "2025-05-10 07:34:53 [INFO]: Epoch 008 - training loss (MAE): 0.3546, validation MSE: 0.1816\n",
      "2025-05-10 07:34:56 [INFO]: Epoch 009 - training loss (MAE): 0.3433, validation MSE: 0.1754\n",
      "2025-05-10 07:34:58 [INFO]: Epoch 010 - training loss (MAE): 0.3387, validation MSE: 0.1772\n",
      "2025-05-10 07:34:58 [INFO]: Finished training. The best model is from epoch#9.\n",
      "2025-05-10 07:34:58 [INFO]: Saved the model to tutorial_results/imputation/saits/20250510_T073432/SAITS.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing mean absolute error: 0.2087\n"
     ]
    }
   ],
   "source": [
    "from pypots.nn.functional import calc_mae\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import SAITS\n",
    "\n",
    "# 创建 SAITS 模型\n",
    "# SAITS 模型的参数可以根据需要进行调整\n",
    "saits = SAITS(\n",
    "    n_steps=processed_dataset['n_steps'],\n",
    "    n_features=processed_dataset['n_features'],\n",
    "    n_layers=1,\n",
    "    d_model=256,\n",
    "    d_ffn=128,\n",
    "    n_heads=4,\n",
    "    d_k=64,\n",
    "    d_v=64,\n",
    "    dropout=0.1,\n",
    "    ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "    # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "    MIT_weight=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=DEVICE,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/saits\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")\n",
    "\n",
    "# 训练阶段，使用训练集和验证集\n",
    "saits.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
    "\n",
    "# 测试阶段，插补缺失值\n",
    "test_set_imputation = saits.impute(dataset_for_testing)\n",
    "\n",
    "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "testing_mae = calc_mae(\n",
    "    test_set_imputation,\n",
    "    test_X_ori,\n",
    "    test_X_indicating_mask,\n",
    ")\n",
    "print(f\"Testing mean absolute error: {testing_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcde375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 插补训练集和验证集\n",
    "train_set_imputation = saits.impute(dataset_for_training)\n",
    "val_set_imputation = saits.impute(dataset_for_validating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a0bc04",
   "metadata": {},
   "source": [
    "### 2.2 将 3D NumPy 数组还原回原始的DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10d4292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataframe(X, labels, sample_ids, scaler, invers_norm = False, n_steps=48):\n",
    "    \"\"\"\n",
    "    Convert 3D NumPy array to a DataFrame with sample_id, timestamp, and original scale features.\n",
    "\n",
    "    Parameters:\n",
    "    - X: 3D NumPy array of shape (n_samples, n_steps, n_features)\n",
    "    - labels: 1D NumPy array of shape (n_samples,) -> labels for each sample\n",
    "    - sample_ids: 1D NumPy array with sample IDs corresponding to each sample\n",
    "    - scaler: Scaler used for normalization (MinMaxScaler/StandardScaler)\n",
    "    - n_steps: Number of time steps (default: 48)\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with sample_id, timestamp, features, and labels\n",
    "    \"\"\"\n",
    "    n_samples, _, n_features = X.shape\n",
    "\n",
    "    assert len(feature_columns) == n_features, \"Number of features in X does not match feature_columns\"\n",
    "    assert len(labels) == n_samples, \"Number of labels does not match number of samples\"\n",
    "    assert len(sample_ids) == n_samples, \"Number of sample IDs does not match number of samples\"\n",
    "\n",
    "    # extract the last timestep record for each sample_id  to get one row per sample,\n",
    "    # using the final timestep’s data (e.g., the last hour if n_steps=48 represents hourly data)\n",
    "\n",
    "    X_last = X[:, -1, :]  # Shape: (n_samples, n_features)\n",
    "\n",
    "    # Inverse normalization\n",
    "    if invers_norm:\n",
    "      X_original = scaler.inverse_transform(X_last)\n",
    "    else:\n",
    "      X_original = X_last\n",
    "\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(X_original, columns=feature_columns)\n",
    "    df['sample_id'] = sample_ids\n",
    "    df['timestamp'] = n_steps - 1  # Last timestep (e.g., 47 if 0-indexed)\n",
    "    df['label'] = labels\n",
    "\n",
    "    # Reorder columns: sample_id, timestamp, features, label\n",
    "    df = df[['sample_id', 'timestamp'] + feature_columns + ['label']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "394b3f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4908, 23) (614, 23) (614, 23)\n"
     ]
    }
   ],
   "source": [
    "df_train_imputed = convert_to_dataframe(train_set_imputation, train_y, train_ids, scaler)\n",
    "df_val_imputed = convert_to_dataframe(val_set_imputation, val_y, val_ids, scaler)\n",
    "df_test_imputed = convert_to_dataframe(test_set_imputation, test_y, test_ids, scaler)\n",
    "\n",
    "# 检查数据集的形状\n",
    "print(df_train_imputed.shape, df_val_imputed.shape, df_test_imputed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "768cf394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>apacheadmissiondx</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>gender</th>\n",
       "      <th>GCS Total</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Motor</th>\n",
       "      <th>Verbal</th>\n",
       "      <th>admissionheight</th>\n",
       "      <th>...</th>\n",
       "      <th>MAP (mmHg)</th>\n",
       "      <th>Invasive BP Diastolic</th>\n",
       "      <th>Invasive BP Systolic</th>\n",
       "      <th>O2 Saturation</th>\n",
       "      <th>Respiratory Rate</th>\n",
       "      <th>Temperature (C)</th>\n",
       "      <th>glucose</th>\n",
       "      <th>FiO2</th>\n",
       "      <th>pH</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3098</td>\n",
       "      <td>47</td>\n",
       "      <td>-0.676732</td>\n",
       "      <td>0.302200</td>\n",
       "      <td>0.918308</td>\n",
       "      <td>0.661468</td>\n",
       "      <td>0.665611</td>\n",
       "      <td>0.399930</td>\n",
       "      <td>0.598729</td>\n",
       "      <td>1.156228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318641</td>\n",
       "      <td>-0.023323</td>\n",
       "      <td>0.197020</td>\n",
       "      <td>-0.199657</td>\n",
       "      <td>1.652006</td>\n",
       "      <td>-0.023425</td>\n",
       "      <td>-0.345354</td>\n",
       "      <td>-0.580239</td>\n",
       "      <td>0.296845</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4221</td>\n",
       "      <td>47</td>\n",
       "      <td>-0.516926</td>\n",
       "      <td>0.375191</td>\n",
       "      <td>-1.088959</td>\n",
       "      <td>0.365205</td>\n",
       "      <td>0.550923</td>\n",
       "      <td>0.398235</td>\n",
       "      <td>0.509197</td>\n",
       "      <td>-1.677569</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.361959</td>\n",
       "      <td>-0.616842</td>\n",
       "      <td>-0.466705</td>\n",
       "      <td>-0.496805</td>\n",
       "      <td>-0.905164</td>\n",
       "      <td>-0.806545</td>\n",
       "      <td>-0.528615</td>\n",
       "      <td>-0.385642</td>\n",
       "      <td>0.150580</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3154</td>\n",
       "      <td>47</td>\n",
       "      <td>-0.490291</td>\n",
       "      <td>0.302200</td>\n",
       "      <td>-1.088959</td>\n",
       "      <td>0.609067</td>\n",
       "      <td>0.571595</td>\n",
       "      <td>0.440095</td>\n",
       "      <td>0.620049</td>\n",
       "      <td>-0.166210</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.836100</td>\n",
       "      <td>-1.012521</td>\n",
       "      <td>-0.289712</td>\n",
       "      <td>0.097490</td>\n",
       "      <td>0.852890</td>\n",
       "      <td>0.109192</td>\n",
       "      <td>-0.374054</td>\n",
       "      <td>-0.387662</td>\n",
       "      <td>0.202498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4041</td>\n",
       "      <td>47</td>\n",
       "      <td>-0.730001</td>\n",
       "      <td>0.302200</td>\n",
       "      <td>-1.088959</td>\n",
       "      <td>0.412285</td>\n",
       "      <td>0.581207</td>\n",
       "      <td>0.403890</td>\n",
       "      <td>0.394523</td>\n",
       "      <td>-1.462887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110087</td>\n",
       "      <td>0.004186</td>\n",
       "      <td>0.014515</td>\n",
       "      <td>0.097490</td>\n",
       "      <td>3.569884</td>\n",
       "      <td>-0.524262</td>\n",
       "      <td>3.389644</td>\n",
       "      <td>-0.319179</td>\n",
       "      <td>-0.208902</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2664</td>\n",
       "      <td>47</td>\n",
       "      <td>-0.829298</td>\n",
       "      <td>0.302200</td>\n",
       "      <td>-1.088959</td>\n",
       "      <td>0.537043</td>\n",
       "      <td>0.564650</td>\n",
       "      <td>0.393065</td>\n",
       "      <td>0.566109</td>\n",
       "      <td>-1.248206</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.702260</td>\n",
       "      <td>-0.682788</td>\n",
       "      <td>-0.732195</td>\n",
       "      <td>-0.793952</td>\n",
       "      <td>1.172537</td>\n",
       "      <td>0.128017</td>\n",
       "      <td>-0.311292</td>\n",
       "      <td>-0.362972</td>\n",
       "      <td>0.115219</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id  timestamp  apacheadmissiondx  ethnicity    gender  GCS Total  \\\n",
       "0       3098         47          -0.676732   0.302200  0.918308   0.661468   \n",
       "1       4221         47          -0.516926   0.375191 -1.088959   0.365205   \n",
       "2       3154         47          -0.490291   0.302200 -1.088959   0.609067   \n",
       "3       4041         47          -0.730001   0.302200 -1.088959   0.412285   \n",
       "4       2664         47          -0.829298   0.302200 -1.088959   0.537043   \n",
       "\n",
       "       Eyes     Motor    Verbal  admissionheight  ...  MAP (mmHg)  \\\n",
       "0  0.665611  0.399930  0.598729         1.156228  ...    0.318641   \n",
       "1  0.550923  0.398235  0.509197        -1.677569  ...   -0.361959   \n",
       "2  0.571595  0.440095  0.620049        -0.166210  ...   -0.836100   \n",
       "3  0.581207  0.403890  0.394523        -1.462887  ...    0.110087   \n",
       "4  0.564650  0.393065  0.566109        -1.248206  ...   -0.702260   \n",
       "\n",
       "   Invasive BP Diastolic  Invasive BP Systolic  O2 Saturation  \\\n",
       "0              -0.023323              0.197020      -0.199657   \n",
       "1              -0.616842             -0.466705      -0.496805   \n",
       "2              -1.012521             -0.289712       0.097490   \n",
       "3               0.004186              0.014515       0.097490   \n",
       "4              -0.682788             -0.732195      -0.793952   \n",
       "\n",
       "   Respiratory Rate  Temperature (C)   glucose      FiO2        pH  label  \n",
       "0          1.652006        -0.023425 -0.345354 -0.580239  0.296845      0  \n",
       "1         -0.905164        -0.806545 -0.528615 -0.385642  0.150580      0  \n",
       "2          0.852890         0.109192 -0.374054 -0.387662  0.202498      0  \n",
       "3          3.569884        -0.524262  3.389644 -0.319179 -0.208902      1  \n",
       "4          1.172537         0.128017 -0.311292 -0.362972  0.115219      0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc13be5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
